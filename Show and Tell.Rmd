# Portfolio Goal Setting:

For my portfolio, I have chosen to create an interactive applet that explains a supervised statistical learning method using real-world data. The applet focuses on linear regression, one of the foundational concepts of statistical modeling. It allows users to upload their own dataset or use a pre-loaded dataset to visualize how linear regression works. Users can interactively adjust the model parameters, see the corresponding changes in the fitted line, and explore the statistical measures like R-squared and residual plots. This applet was developed using the R programming language and the Shiny package.

# Course Objectives

## **Objective 1 :** Describe probability as a foundation of statistical modeling, including inference and maximum likelihood estimation

The provided example demonstrates the application of a linear regression model to predict car miles per gallon (mpg) based on predictor variables such as car weight (wt), horsepower (hp), and quarter-mile time (qsec) using the 'mtcars' dataset. The model is fitted using maximum likelihood estimation (MLE), which leverages probability to quantify uncertainty and estimate model parameters that best explain the observed data. Probability serves as the foundation for statistical modeling, enabling us to understand uncertainty and make predictions based on data.

```{r}
# Load necessary libraries
library(stats)
library(GGally)
library(tidyverse)
library(tidymodels)
#plots to check the relationship between response and predictor variables
mtcars %>% 
  select(mpg,wt,hp,qsec,cyl,disp,drat) %>% 
  ggpairs()
```

In this example the important predictor variable for mpg is wt with a pvalue \<0.05.As wt increases the mpg decreases.One unit increase in wt leads to 4.38 decrease in mpg. The model becomes mpg = 16.53 - 4.38 (wt). The model estimates are the maximum likelihood estimators of the multiple linear regression coefficients

```{r}
# Created a parsnip specification for a linear model
lm_spec <- linear_reg() %>%
set_mode("regression") %>%
set_engine("lm")

lm_spec
# Fit our specification to our data
#To check which predictor variables are associated with the response
#We use the linearly related predictor variables 
mlr_mod <- lm_spec %>% 
fit(mpg ~ wt+hp+qsec+disp+drat, data = mtcars)
# View our model output
tidy(mlr_mod)

```

To assess our model fit, we can use $R^2$ (the coefficient of determination), the proportion of variability in the response variable that is explained by the explanatory variable.With R-squared of 0.84 it means that, approximately 84% of the variability in the dependent variable mpg can be explained by the independent variables wt

```{r}
glance(mlr_mod)
```

## **Objective 2 :** Determine and apply the appropriate generalized linear model for a specific data context

### Logistic Regresion GLM

This example uses the "Default" dataset to perform a logistic regression analysis. The objective is to predict the likelihood of default (binary outcome) based on the predictor variables: "student," "balance," and "income." The code first loads the necessary libraries and displays a summary of the dataset.

After that, it fits a multiple logistic regression model using the "glm" function from the "tidyverse" package. The results are then presented, showing the coefficients and their statistics. To make the interpretation easier, the exponentiated coefficients are also displayed.

Next, the code creates two plots to assess the model's fit. The first plot shows the deviance residuals against the fitted values, helping us visualize the distribution of residuals and identify any patterns or trends. The second plot displays the deviance residuals against the row numbers, allowing us to detect potential outliers or influential observations.

Overall, this code provides a comprehensive analysis of the logistic regression model a member of generalized linear models, from fitting the model to evaluating its fit using residual plots. It helps understand how the predictor variables contribute to the likelihood of default in the "Default" dataset.

```{r}
# Load necessary libraries
library(ISLR2)       # For accessing the "Default" dataset
library(tidyverse)   # For data manipulation and visualization
library(tidymodels)  # For building generalized linear models

# Load the "Default" dataset
data <- ISLR2::Default

# View summary of the dataset
summary(data)

# Find the total number of observations in the dataset
nrow(data)

# Fit the multiple logistic regression model
mult_log_mod <- glm(default ~ student + balance + income, data = data, family = "binomial")

# Display the coefficients and their statistics
tidy(mult_log_mod)

# Display the exponentiated coefficients for interpretation
tidy(mult_log_mod, exponentiate = TRUE) %>% 
  knitr::kable(digits = 3)

# To store residuals and create a row number variable
mult_log_aug <- augment(mult_log_mod, type.predict = "response", 
                        type.residuals = "deviance") %>% 
                        mutate(id = row_number())

# Assess model fit

# Plot residuals vs fitted values
ggplot(data = mult_log_aug, aes(x = .fitted, y = .resid)) + 
geom_point() + 
geom_hline(yintercept = 0, color = "red") + 
labs(x = "Fitted values", 
     y = "Deviance residuals", 
     title = "Deviance residuals vs. fitted")

# Plot residuals vs row number
ggplot(data = mult_log_aug, aes(x = id, y = .resid)) + 
geom_point() + 
geom_hline(yintercept = 0, color = "red") + 
labs(x = "id", 
     y = "Deviance residuals", 
     title = "Deviance residuals vs. id")

```

## **Objective3 :** Conduct model selection for a set of candidate models

### Subset selection method

This example below performs best subsets regression on the "mtcars" dataset to find the best model with the optimal number of predictor variables. It explores different metrics, such as R-squared, Adjusted R-squared, Cp, and BIC, to evaluate the performance of models with varying numbers of predictors. The code generates plots to visualize the relationships between these metrics and the number of variables in the model. Ultimately, it identifies the model with the highest Adjusted R-squared and the model with the minimum Cp and BIC values. Additionally, it retrieves the coefficients of the model with 6 variables, providing insights into the significant predictors for that model.

```{r}
# Load required libraries
library(ISLR2)
library(leaps)

# Check column names, dimensions, and missing values in mtcars dataset
names(mtcars)
dim(mtcars)
sum(is.na(mtcars$mpg))

# Perform best subsets regression with up to 19 variables
regfit.full <- regsubsets(mpg ~ ., data = mtcars, nvmax = 19)

# Summarize the results of the best subsets regression
reg.summary <- summary(regfit.full)

# View the R-squared values for different models with varying number of variables
reg.summary$rsq

# Create a 2x2 plot to visualize the relationship between RSS and number of variables,
# and between Adjusted R-squared and number of variables
par(mfrow = c(2, 2))
plot(reg.summary$rss, xlab = "Number of Variables", ylab = "RSS", type = "l")
plot(reg.summary$adjr2, xlab = "Number of Variables", ylab = "Adjusted RSq", type = "l")

# Identify the index of the model with the highest Adjusted R-squared value and plot a red point
# to mark the maximum value
which.max(reg.summary$adjr2)
points(11, reg.summary$adjr2[11], col = "red", cex = 2, pch = 20)

# Create a plot to visualize the relationship between Cp and number of variables,
# and identify the index of the model with the minimum Cp value and mark it with a red point
plot(reg.summary$cp, xlab = "Number of Variables", ylab = "Cp", type = "l")
which.min(reg.summary$cp)
points(10, reg.summary$cp[10], col = "red", cex = 2, pch = 20)

# Identify the index of the model with the minimum BIC value and plot a red point
# to mark the minimum value
which.min(reg.summary$bic)
plot(reg.summary$bic, xlab = "Number of Variables", ylab = "BIC", type = "l")
points(6, reg.summary$bic[6], col = "red", cex = 2, pch = 20)

# Create plots to visualize the relationship between R-squared, Adjusted R-squared, Cp, and BIC
# for all models in the best subsets regression
plot(regfit.full, scale = "r2")
plot(regfit.full, scale = "adjr2")
plot(regfit.full, scale = "Cp")
plot(regfit.full, scale = "bic")

# View the coefficients of the model with 6 variables
coef(regfit.full, 6)


```

```{r}
 par (mfrow = c(2, 2))
plot (reg.summary$rss , xlab = " Number of Variables ",
ylab = " RSS ", type = "l")
plot (reg.summary$adjr2 , xlab = " Number of Variables ",
ylab = " Adjusted RSq ", type = "l")
```

### Forward and backward selection

This following example performs forward stepwise regression and backward stepwise regression using best subsets on the "mtcars" dataset with the response variable "mpg" and predictor variables from the dataset. The regsubsets function from the "leaps" package is used to perform the best subsets regression.

After fitting the forward and backward stepwise regression models, the code then uses the summary function to view the results, which include information about the selected variables and their coefficients for each model with different numbers of variables (up to 19).

Finally, the code retrieves the coefficients of the model with 7 variables from the full model, the forward stepwise model, and the backward stepwise model, respectively.

In summary, this code conducts model selection by exploring different combinations of predictor variables and provides insights into the best models with a specific number of variables, helping to identify the most relevant predictors for explaining the variation in the "mpg" response variable.

```{r}
# Perform forward stepwise regression using best subsets
regfit.fwd <- regsubsets(mpg ~ ., data = mtcars, nvmax = 19, method = "forward")

# View summary of forward stepwise regression results
summary(regfit.fwd)

# Perform backward stepwise regression using best subsets
regfit.bwd <- regsubsets(mpg ~ ., data = mtcars, nvmax = 19, method = "backward")

# View summary of backward stepwise regression results
summary(regfit.bwd)

# Retrieve coefficients of the model with 7 variables from the full model
coef(regfit.full, 7)

# Retrieve coefficients of the model with 7 variables from the forward stepwise model
coef(regfit.fwd, 7)

# Retrieve coefficients of the model with 7 variables from the backward stepwise model
coef(regfit.bwd, 7)

```

## **Objective 4 :** Communicate the results of statistical models to a general audience

The applet is designed to be user-friendly and accessible to a general audience. It provides clear visualizations, easy-to-understand sliders for model parameters, and interactive components that engage users in the learning process. By being intuitive and informative, the applet effectively communicates the concepts of linear regression and its results to a broader audience.

## **Objective 5 :** Use programming software (i.e., R) to fit and assess statistical models

This applet is entirely built using R and the Shiny package. It showcases my proficiency in using R for fitting and assessing statistical models. The applet employs various R packages for data manipulation, model fitting, and visualization. Additionally, the applet demonstrates my ability to present statistical concepts in an interactive and engaging way using R programming.

# Reflection and Learning growth

Reflection on Learning and Growth: Throughout the semester, I have gained a deeper understanding of probability and its essential role in statistical modeling. I initially struggled with grasping the concept of maximum likelihood estimation, but through practice, in-class activities, and discussions with peers, I now feel much more confident in this area. Additionally, I have developed a solid grasp of generalized linear models and their applications in different data contexts.

I found model selection to be challenging, as it required balancing between simplicity and accuracy in the models. However, participating in class activities and mini-competitions helped me develop a systematic approach to model selection and identify the trade-offs involved.

Regarding communication, I have made an effort to improve my ability to present complex statistical concepts to a general audience effectively. The interactive applet I created for this portfolio is a testament to that growth. I learned how to simplify complex ideas and create engaging visualizations to facilitate understanding.

As for using programming software, I came into the course with some experience in R, but I am delighted with how much I've learned and utilized R to fit, assess, and visualize various statistical models.

Reflection on Active Participation in the Course Community: Throughout the semester, I actively participated in the course community by regularly attending class, actively engaging in team discussions, and contributing to online forums. I enjoyed collaborating with my peers on mini-competitions and in-class activities, as it provided an opportunity to learn from different perspectives and approaches.

To foster a supportive community, I made an effort to be responsive to my peers' questions and encouraged collaboration within our teams. I also shared resources and helpful materials whenever I came across them, which contributed to a positive learning environment.

Overall, this course has been a journey of growth and self-discovery. I am grateful for the supportive community and the opportunity to apply my knowledge to real-world data through various projects and competitions. I feel much more confident in my statistical modeling abilities and look forward to applying these skills in future endeavors.
